{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"kwic.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"qFq-8WhozlB_","colab_type":"text"},"source":["# 0 - Preflight checks\n","\n","Import all of the relevant packages that will be used later.\n","\n","Functions that will be used for cleaning in the main part of the code."]},{"cell_type":"code","metadata":{"id":"9yS2KhTnzlCB","colab_type":"code","colab":{}},"source":["import time\n","import csv\n","import re\n","import string\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.collocations import *\n","from nltk.corpus import wordnet as wn\n","\n","import spacy\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUMNJZz3zlCH","colab_type":"code","colab":{}},"source":["def encode_volid(volid, direction='path'):\n","    '''\n","    Transform htid into filename encoded version and vice versa\n","    '''\n","    encoding_fixes = {'+':':', '=':'/'}\n","    if direction=='path':\n","        encoding_fixes = {v:k for k,v in encoding_fixes.items()}\n","    for key in encoding_fixes:\n","        volid = volid.replace(key, encoding_fixes[key])\n","    return(volid)\n","\n","#Reformat page number to find corresponding json files\n","def find_page(page_n):\n","    n = len(page_n)\n","    N = 8 - n\n","    zeros = \"\"\n","    while N > 0:\n","        zeros += \"0\"\n","        N -= 1\n","    return zeros + page_n\n","\n","\n","# Prepare list of urbanterms\n","with open(\"urbanterms.csv\", 'r', encoding='utf-8') as csv_file:\n","    dict_csv = csv.DictReader(csv_file)\n","    urbanterms = [row[\"term\"] for row in dict_csv]\n","\n","\n","# Prepare list of stop words\n","stoplist_file = 'stopwords-underwood-goldstone.txt'\n","stoplist = [line.strip() for line in open(stoplist_file)]\n","stoplist = set(stoplist)\n","\n","\n","def cleanText(text):\n","    \"\"\" \n","    A quick cleaner function to remove any unwanted noise from novels.\n","    \"\"\"\n","    #strip punctuation\n","    text = \"\".join(l for l in text if l not in string.punctuation)\n","    text = re.sub(r\"[^\\w\\d'\\s]+\",'',text)\n","    # whitespace\n","    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"  \",\" \")\n","    # lowercase\n","    text = text.lower()\n","    return text\n","\n","\n","def context_extraction(tags, keyword):\n","    numwords = 5\n","    context = []\n","    for x in [x for (x, y) in enumerate(tags) if keyword == y[0]]:\n","        context.extend(tags[x-numwords:x])\n","        context.extend(tags[x+1:x+numwords+1])    \n","    return context\n","\n","\n","def spacyTags(token):\n","    \"\"\"\n","    Select only those spaCy token attributes that we want to work with\n","    \"\"\"\n","    tags = (token.lemma_, token.pos_)\n","    return tags"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8unqJKRtzlCL","colab_type":"text"},"source":["# 1 - Create htid - page dictionary\n","\n","From csv file create a dictionary with htids as keys and lists of pages' numbers"]},{"cell_type":"code","metadata":{"id":"eLUBIr3_zlCM","colab_type":"code","colab":{}},"source":["htids_pages = dict()\n","with open(\"pagesdata.csv\", 'r', encoding='utf-8') as csv_file:\n","        dict_csv = csv.DictReader(csv_file)\n","        for row in dict_csv:\n","            if row[\"htid\"] not in htids_pages.keys():\n","                htids_pages[row[\"htid\"]] = set()\n","                htids_pages[row[\"htid\"]].add(str(row[\"page\"]))\n","            else:\n","                htids_pages[row[\"htid\"]].add(str(row[\"page\"]))\n","\n","print(\"N. scifi novels:\", len(htids_pages))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pTSn8vdhzlCP","colab_type":"text"},"source":["# 2 - Create dictionary with keywords, htids and contexts\n","\n","Point this bit in the direction of the novel you want to study.\n","\n","spaCy is happier to tag each page than the whole book."]},{"cell_type":"code","metadata":{"id":"iMVX8t4czlCQ","colab_type":"code","colab":{}},"source":["nlp = spacy.load('en', disable=[\"ner\", \"parser\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoaWTls4zlCT","colab_type":"code","colab":{}},"source":["kwic_dict = dict()\n","for keyword in urbanterms:\n","    kwic_dict[keyword] = dict()\n","    \n","    start = time.perf_counter()\n","    for htid, pages in htids_pages.items():\n","        kwic_dict[keyword][htid] = dict()\n","        kwic_dict[keyword][htid][\"NOUNS\"] = dict()\n","        kwic_dict[keyword][htid][\"ADJS\"] = dict()\n","        kwic_dict[keyword][htid][\"VERBS\"] = dict()\n","    \n","        for page_n in pages:        \n","            page = find_page(page_n)\n","            with open(f\"/media/secure_volume/workset/{encode_volid(htid)}/{page}.txt\") as infile:\n","                page_text = infile.read()\n","                infile.close()\n","                clean = cleanText(page_text) #cleaning\n","                doc = nlp(clean) #tokenization\n","                end = len(doc) - 5 + 1\n","                tags = [spacyTags(token) for token in doc[5:end] if token not in stoplist]\n","                contexts = context_extraction(tags, keyword)\n","                    \n","                for word in contexts:\n","                    if word[1] == \"NOUN\":\n","                        if word[0] not in kwic_dict[keyword][htid][\"NOUNS\"].keys():\n","                            kwic_dict[keyword][htid][\"NOUNS\"][word[0]] = 1\n","                        else:\n","                            kwic_dict[keyword][htid][\"NOUNS\"][word[0]] += 1\n","\n","                    elif word[1] == \"VERB\":\n","                        if word[0] not in kwic_dict[keyword][htid][\"VERBS\"].keys():\n","                            kwic_dict[keyword][htid][\"VERBS\"][word[0]] = 1\n","                        else:\n","                            kwic_dict[keyword][htid][\"VERBS\"][word[0]] += 1\n","\n","                    elif word[1] == \"ADJ\":\n","                        if word[0] not in kwic_dict[keyword][htid][\"ADJS\"].keys():\n","                            kwic_dict[keyword][htid][\"ADJS\"][word[0]] = 1\n","                        else:\n","                            kwic_dict[keyword][htid][\"ADJS\"][word[0]] += 1\n","        \n","    end = time.perf_counter()\n","    print(end-start)           "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEpNf_L-zlCb","colab_type":"text"},"source":["# 3 - Create csv file to store the results\n","\n","This section converts the tagged novel into a neat, easy-to-read dataframe format using pandas."]},{"cell_type":"code","metadata":{"id":"D_hUc_s4zlCb","colab_type":"code","colab":{}},"source":["with open(os.path.join(bigDir,\"kwicdata.csv\"),'w', encoding='utf-8', newline='') as kwicdata:\n","    writer = csv.writer(kwicdata)\n","    writer.writerow((\"keyword\", \"htid\", \"POS\", \"word\", \"count\"))\n","    for keyword, htids in kwic_dict.items():\n","        for htid, POSS in htid.items():\n","            for POS, words in POSS.items():\n","                for word, count in words.items():\n","                    writer.writerow((htid, POS, word, count))\n"],"execution_count":0,"outputs":[]}]}